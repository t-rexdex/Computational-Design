{"cells":[{"cell_type":"code","execution_count":null,"id":"de649b96","metadata":{"id":"de649b96"},"outputs":[],"source":["import numpy as np\n","from sympy import Matrix\n","from sympy import symbols\n","from sympy import lambdify\n","from sympy.solvers import solve"]},{"cell_type":"markdown","source":["## Task 1: Golden ratio line search"],"metadata":{"id":"uKCAiNxPUfm4"},"id":"uKCAiNxPUfm4"},{"cell_type":"code","execution_count":null,"id":"47dce12d","metadata":{"id":"47dce12d"},"outputs":[],"source":["def gsSearch(f,x=0,d=0,eps=10e-5):\n","  ''' perform golden ratio search\n","      input:\n","        f - the function to be evaluated\n","        x - current iteration optimization variable\n","        d - direction to search\n","        eps - small number that serves to terminate the search\n","      output:\n","        alpha - parameter to multiply by d to calculate the next x\n","  '''\n","  \n","#################################\n","# Start entering your code here #\n","#################################\n","\n","  if x.ndim != 1:\n","    print ('The array x must be a one dimensional matrix')\n","\n","  # Phase I - Bracketing\n","  delta = 0.05\n","  alpha = [0]\n","  \n","  tau = 0.618\n","  tauSumList = [0]\n","\n","  for q in np.arange(0, 50):\n","    tauSumList.append((tau+1)**q*delta)\n","\n","  for q in np.arange(0, 50):\n","    if q > 0:\n","      if f(x+alpha[q]*d)<f(x+alpha[q-1]*d):\n","        alpha.append(tauSumList[q+1])\n","      else:\n","        break\n","    else:\n","      alpha.append(tauSumList[q+1])\n","\n","  # Phase II\n","  if len(alpha)>=3:\n","    xA = alpha[-3]\n","  else:\n","    xA = 0\n","  xD = alpha[-1]\n","\n","  i = 0\n","  while (xD-xA)>eps and i<10000:\n","    xB = xA + (1-tau)*(xD-xA)\n","    xC = xA + tau*(xD-xA)\n","    if f(x+xB*d) <= f(x+xC*d):\n","      xD = xC\n","    elif f(x+xB*d) > f(x+xC*d):\n","      xA = xB\n","    i += 1\n","  alpha = (xD+xA)/2.\n","\n","#################################\n","# End of your code              #\n","#################################\n","\n","  return alpha"]},{"cell_type":"markdown","source":["## Task 2: Steepest descent"],"metadata":{"id":"n87x2E3XYYzB"},"id":"n87x2E3XYYzB"},{"cell_type":"code","source":["def sd(f, x=0, der=0, eps=10e-5):\n","  ''' Steepest descent\n","      input:\n","        f - objective function as a python function\n","        x - initial guess of the optimal solution\n","        der - analytical derivative of the objective function as a python function\n","        eps - small number to terminate the algorithm\n","      output:\n","        x_out - solution of x from all the iteration (not just the optimal value)\n","  '''\n","\n","  \n","#################################\n","# Start entering your code here #\n","#################################\n","\n","  if x.ndim != 1:\n","    print ('The array x must be a one dimensional matrix')\n","\n","  i=0\n","  c = der(x) # initialize the gradient at x0\n","  x_out = x # first x solution recorded\n","\n","  while np.linalg.norm(c)>eps and i<10000:\n","    c = der(x) # differentiate\n","    cnorm = c / np.linalg.norm(c) # normalize the negative of the gradient\n","    d = -cnorm.transpose()[0] # transpose to n by 1\n","    alpha_s = gsSearch(f, x, d, eps) # find alpha using gsSearch\n","\n","    x = x+alpha_s*d # update x\n","\n","    x_out = np.vstack([x_out, x])\n","    i+=1\n","\n","  \n","#################################\n","# End of your code              #\n","#################################\n","\n","  return x_out"],"metadata":{"id":"rGlOSRbOLdHD"},"id":"rGlOSRbOLdHD","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Task 3: Conjugate Gradient"],"metadata":{"id":"G5bwtfE8IoSE"},"id":"G5bwtfE8IoSE"},{"cell_type":"code","source":["def cg(f, x=0, der=0, eps=10e-5):\n","  ''' Conjugate gradient\n","      input:\n","        f - objective function as a python function\n","        x - initial guess of the optimal solution\n","        der - analytical derivative of the objective function as a python function\n","        eps - small number to terminate the algorithm\n","      output:\n","        x_out - solution of x from all the iteration (not just the optimal value)\n","  '''\n","\n","#################################\n","# Start entering your code here #\n","#################################\n","\n","  if x.ndim != 1:\n","    print ('The array x must be a one dimensional matrix')\n","\n","  i=0\n","  x_out = x\n","\n","  while np.linalg.norm(der(x))>eps and i<1000:\n","\n","    if i == 0: # first iteration\n","      c = der(x).transpose()[0]\n","      d = -c\n","    else: # all other iterations\n","      c_old = c\n","      c = der(x).transpose()[0]\n","      beta = np.matmul(c.transpose(),c)/np.matmul(c_old.transpose(),c_old)\n","\n","      d = -c+beta*d\n","\n","    alpha_s = gsSearch(f, x, d, eps)\n","\n","    x = x+alpha_s*d\n","\n","    x_out = np.vstack([x_out, x])\n","    \n","    i+=1\n","  \n","#################################\n","# End of your code              #\n","#################################\n","\n","  return x_out"],"metadata":{"id":"hm8KvQ44Inh4"},"id":"hm8KvQ44Inh4","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Task 4: Modified Newton Method"],"metadata":{"id":"JgiqVbiXNZVk"},"id":"JgiqVbiXNZVk"},{"cell_type":"code","source":["def mn(f, x=0, der=0, hes=0, eps=10e-5):\n","  ''' Modified Newton Method\n","      input:\n","        f - objective function as a python function\n","        x - initial guess of the optimal solution\n","        der - analytical derivative of the objective function as a python function\n","        hes - analytical hessian of the objective function as a python function\n","        eps - small number to terminate the algorithm\n","      output:\n","        x_out - solution of x from all the iteration (not just the optimal value)\n","  '''\n","\n","#################################\n","# Start entering your code here #\n","#################################\n","\n","  if x.ndim != 1:\n","    print ('The array x must be a one dimensional matrix')\n","\n","  i=0\n","  x_out = x\n","\n","  while np.linalg.norm(der(x))>eps and i<10000:\n","      c = der(x)\n","\n","      d = np.linalg.solve(hes(x), -c)\n","      d = d.transpose()[0]\n","      alpha_s = gsSearch(f, x, d, eps)\n","\n","      x = x+alpha_s*d\n","\n","      x_out = np.vstack([x_out, x])\n","      \n","      i+=1\n","  \n","#################################\n","# End of your code              #\n","#################################\n","\n","  return x_out"],"metadata":{"id":"jDTMxKiMNT9l"},"id":"jDTMxKiMNT9l","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test case 1: Quadratic function"],"metadata":{"id":"U7jJ9lb2Mkqn"},"id":"U7jJ9lb2Mkqn"},{"cell_type":"code","source":["#######################################\n","# Test case 1\n","# Do not modify this code\n","# You may run it to test your algorithm\n","#######################################\n","\n","# Objective function\n","def testFun(x):\n","  return x[0]**2+x[1]**2\n","\n","# Derivative of the objective function\n","def testFunDer(x):\n","  return np.array([[2*x[0]], [2*x[1]]])\n","\n","# Derivative of the objective function\n","def testFunHes(x):\n","  return np.array([[2, 0], [0, 2]])\n","\n","#######################################\n","# execution of your optimization code #\n","#######################################\n","x0 = np.array([-25., 75.])\n","\n","x_out_sd = sd(testFun,x0,testFunDer,0.00001)\n","print('Steepest descent gives: ' + str(x_out_sd[-1]))\n","\n","x_out_cg = cg(testFun,x0,testFunDer,0.00001)\n","print('Conjugate gradient gives: ' + str(x_out_cg[-1]))\n","\n","x_out_mn = mn(testFun,x0,testFunDer,testFunHes, 0.00001)\n","print('Modified Newton gives: ' + str(x_out_mn[-1]))\n","\n","# Check your code against scipy optimize\n","from scipy.optimize import minimize\n","res = minimize(testFun, x0)\n","print('Scipy gives: ' + str(res.x))\n"],"metadata":{"id":"IpUwi8TdW_Sx"},"id":"IpUwi8TdW_Sx","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#######################################\n","# Test case 1\n","# Do not modify this code\n","# You may run it to test your algorithm\n","#######################################\n","\n","# Plot the contour and the optimization iteration\n","import matplotlib.pyplot as plt\n","\n","n = 1000 # calculate for n data points \n","val_x = np.linspace(-100.,100., n)  \n","\n","# meshgrid allows us to weave these two variables to arrive at n^2 points for calculation\n","optVar_x0, optVar_x1 = np.meshgrid(val_x, val_x)\n","\n","# Plot the contour map\n","plt.contour(optVar_x0, optVar_x1, testFun([optVar_x0, optVar_x1]))\n","# Scatter plot of the iteration of x\n","plt.scatter(x_out_sd[:,0],x_out_sd[:,1], c='b')\n","plt.plot   (x_out_sd[:,0],x_out_sd[:,1], c='b')\n","plt.scatter(x_out_cg[:,0],x_out_cg[:,1], c='r')\n","plt.plot   (x_out_cg[:,0],x_out_cg[:,1], c='r')\n","plt.scatter(x_out_mn[:,0],x_out_mn[:,1], c='g')\n","plt.plot   (x_out_mn[:,0],x_out_mn[:,1], c='g')\n","\n","plt.show()"],"metadata":{"id":"RYL45M6bXTr0"},"id":"RYL45M6bXTr0","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test case 2: Box optimization"],"metadata":{"id":"FkK4h358MncZ"},"id":"FkK4h358MncZ"},{"cell_type":"code","source":["#######################################\n","# Test case 2\n","# Do not modify this code\n","# You may run it to test your algorithm\n","#######################################\n","\n","v_req = 10**6\n","\n","from sympy import symbols\n","from sympy import lambdify\n","from sympy import Matrix\n","\n","# Define optimization variables\n","a, b = symbols('a b')\n","\n","# Define the objective to minimize\n","box  = a*b+2*v_req/b+2*v_req/a \n","dBox = Matrix([box.diff(x) for x in [a,b]]) # Calculate gradient\n","ddBox = Matrix([[y.diff(x) for y in dBox] for x in [a,b]]) # Calculate hessian\n","\n","# lambdify the objective function\n","boxF = lambdify([a,b], box)\n","dBoxF = lambdify([a,b], dBox)\n","ddBoxF = lambdify([a,b], ddBox)\n","\n","# Wrapper functions\n","def boxFW (x):\n","  return boxF(x[0],x[1])\n","\n","def dBoxFW (x):\n","  return dBoxF(x[0],x[1])\n","\n","def ddBoxFW (x):\n","  return ddBoxF(x[0],x[1])\n","\n","#######################################\n","# execution of your optimization code #\n","#######################################\n","x0 = np.array([60., 200.])\n","\n","x_out_sd = sd(boxFW,x0,dBoxFW,0.00001)\n","print('Steepest descent gives: ' + str(x_out_sd[-1]))\n","\n","x_out_cg = cg(boxFW,x0,dBoxFW,0.00001)\n","print('Conjugate gradient gives: ' + str(x_out_cg[-1]))\n","\n","x_out_mn = mn(boxFW,x0,dBoxFW,ddBoxFW,0.00001)\n","print('Modified Newton gives: ' + str(x_out_mn[-1]))\n","\n","# Check your code against scipy optimize\n","from scipy.optimize import minimize\n","res = minimize(boxFW, x0)\n","print('Scipy gives: ' + str(res.x))\n"],"metadata":{"id":"dSsRhz1i_doh"},"id":"dSsRhz1i_doh","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#######################################\n","# Test case 2\n","# Do not modify this code\n","# You may run it to check your algorithm\n","#######################################\n","\n","n = 1000 # calculate for n data points \n","val_x = np.linspace(10., 220., 100)  \n","\n","# meshgrid allows us to weave these two variables to arrive at n^2 points for calculation\n","optVar_x0, optVar_x1 = np.meshgrid(val_x, val_x)\n","\n","import matplotlib.pyplot as plt\n","# initialize a figure container\n","fig = plt.figure(figsize=(8, 6))\n","\n","# Plot the contour map to show objective landscape\n","plt.contour(optVar_x0, optVar_x1, boxFW([optVar_x0, optVar_x1]))\n","# Scatter plot of the iteration of x_star values\n","\n","plt.scatter(x_out_sd[:,0],x_out_sd[:,1], c='b')\n","plt.plot   (x_out_sd[:,0],x_out_sd[:,1], c='b')\n","plt.scatter(x_out_cg[:,0],x_out_cg[:,1], c='r')\n","plt.plot   (x_out_cg[:,0],x_out_cg[:,1], c='r')\n","plt.scatter(x_out_mn[:,0],x_out_mn[:,1], c='g')\n","plt.plot   (x_out_mn[:,0],x_out_mn[:,1], c='g')\n","\n","plt.xlim([50,150])\n","plt.ylim([100,225])\n","plt.show()"],"metadata":{"id":"8AjtlVW_HPEh"},"id":"8AjtlVW_HPEh","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test case 3: Wire optimization"],"metadata":{"id":"2aF1k6c8Mx2h"},"id":"2aF1k6c8Mx2h"},{"cell_type":"code","source":["#######################################\n","# Test case 3\n","# Do not modify this code\n","# You may run it to test your algorithm\n","#######################################\n","\n","# Set all the parameters\n","y_start = 50; x_start = 50; x_end = 50; y_end = 350; a = 500; b = 400; t = 75\n","\n","from sympy import symbols\n","from sympy import lambdify\n","from sympy import Matrix\n","# Define optimization variables\n","y1, y2 = symbols('y1 y2')\n","\n","# Define the objective to minimize\n","length  = ((x_start-a)**2+(y_start-y1)**2)**0.5 + \\\n","          ((y1-y2)**2+t**2)**0.5 + \\\n","          ((a-x_end)**2+(y2-y_end)**2)**0.5\n","\n","# Calculate gradient then cast to sympy matrix\n","dLength = Matrix([length.diff(x) for x in [y1,y2]]) \n","ddLength = Matrix([[y.diff(x) for y in dLength] for x in [y1,y2]]) # Calculate hessian\n","\n","\n","# lambdify the objective function and the derivative\n","lengthF     = lambdify([y1,y2], length)\n","dLengthF     = lambdify([y1,y2], dLength)\n","ddLengthF     = lambdify([y1,y2], ddLength)\n","\n","# define wrapper functions\n","def lengthFW (x):\n","  return lengthF(x[0],x[1])\n","\n","def dLengthFW (x):\n","  return dLengthF(x[0],x[1])\n","\n","def ddLengthFW (x):\n","  return ddLengthF(x[0],x[1])\n","\n","\n","#######################################\n","# execution of your optimization code #\n","#######################################\n","x0 = np.array([20., 200.])\n","x_out_sd = sd(lengthFW,x0,dLengthFW,0.00001)\n","print('Steepest descent gives: ' + str(x_out_sd[-1]))\n","\n","x_out_cg = cg(lengthFW,x0,dLengthFW,0.00001)\n","print('Conjugate gradient gives: ' + str(x_out_cg[-1]))\n","\n","x_out_mn = mn(lengthFW,x0,dLengthFW,ddLengthFW,0.00001)\n","print('Modified Newton gives: ' + str(x_out_mn[-1]))\n","\n","# Check your code against scipy minimize\n","from scipy.optimize import minimize\n","res = minimize(lengthFW, x0)\n","print('Scipy gives: ' + str(res.x))"],"metadata":{"id":"J0ZjPll-LwiV"},"id":"J0ZjPll-LwiV","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#######################################\n","# Test case 3\n","# Do not modify this code\n","# You may run it to check your algorithm\n","#######################################\n","\n","import matplotlib.pyplot as plt\n","\n","# calculate for n data points \n","n = 1000 \n","val_x = np.linspace(0., b, n)  \n","\n","# meshgrid allows us to weave these two variables to arrive at n^2 points for calculation\n","optVar_x0, optVar_x1 = np.meshgrid(val_x, val_x)\n","\n","# initialize a figure container\n","fig = plt.figure(figsize=(8, 6))\n","\n","# plot contour to display objective landscape\n","plt.contour(optVar_x0, optVar_x1, lengthFW([optVar_x0, optVar_x1]))\n","\n","# plot the iteration x_star values\n","\n","plt.scatter(x_out_sd[:,0],x_out_sd[:,1], c='b')\n","plt.plot   (x_out_sd[:,0],x_out_sd[:,1], c='b')\n","plt.scatter(x_out_cg[:,0],x_out_cg[:,1], c='r')\n","plt.plot   (x_out_cg[:,0],x_out_cg[:,1], c='r')\n","plt.scatter(x_out_mn[:,0],x_out_mn[:,1], c='g')\n","plt.plot   (x_out_mn[:,0],x_out_mn[:,1], c='g')\n","\n","\n","plt.xlim([0,300])\n","plt.ylim([100,250])\n","\n","plt.show()"],"metadata":{"id":"D53VorBS3lvn"},"id":"D53VorBS3lvn","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test case 4: Rosenbrock function\n"],"metadata":{"id":"EzGqgn6qYklJ"},"id":"EzGqgn6qYklJ"},{"cell_type":"code","source":["#######################################\n","# Test case 4\n","# Do not modify this code\n","# You may run it to test your algorithm\n","#######################################\n","\n","# Set all the parameters\n","a = 0; b = 100;\n","\n","from sympy import symbols\n","from sympy import lambdify\n","from sympy import Matrix\n","# Define optimization variables\n","x, y = symbols('x y')\n","\n","# Define the objective to minimize\n","rosen = (a-x)**2+b*(y-x**2)**2\n","dRosen = Matrix([rosen.diff(x) for x in [x,y]]) \n","ddRosen = Matrix([[y.diff(x) for y in dRosen] for x in [x,y]]) # Calculate hessian\n","\n","\n","# lambdify the objective function and the derivative\n","rosenF    = lambdify([x,y], rosen)\n","dRosenF   = lambdify([x,y], dRosen)\n","ddRosenF  = lambdify([x,y], ddRosen)\n","\n","# define wrapper functions\n","def rosenFW (x):\n","  return rosenF(x[0],x[1])\n","\n","def dRosenFW (x):\n","  return dRosenF(x[0],x[1])\n","\n","def ddRosenFW (x):\n","  return ddRosenF(x[0],x[1])\n","\n","#######################################\n","# execution of your optimization code #\n","#######################################\n","x0 = np.array([-25., 300.])\n","x_out_sd = sd(rosenFW,x0,dRosenFW,0.0000001)\n","print('Steepest descent gives: ' + str(x_out_sd[-1]))\n","x_out_cg = cg(rosenFW,x0,dRosenFW,0.0000001)\n","print('Conjugate gradient gives: ' + str(x_out_cg[-1]))\n","x_out_mn = mn(rosenFW,x0,dRosenFW,ddRosenFW,0.0000001)\n","print('Modified Newton gives: ' + str(x_out_mn[-1]))\n","\n","# Check your code against scipy optimize\n","from scipy.optimize import minimize\n","res = minimize(testFun, x0)\n","print('Scipy gives: ' + str(res.x))"],"metadata":{"id":"TUObxfAiYuX4"},"id":"TUObxfAiYuX4","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#######################################\n","# Test case 4\n","# Do not modify this code\n","# You may run it to check your algorithm\n","#######################################\n","\n","import matplotlib.pyplot as plt\n","\n","# calculate for n data points \n","n = 10\n","\n","val_x0 = np.linspace(-20., 20., n)\n","val_x1 = np.linspace(-200., 400., n)\n","\n","# meshgrid allows us to weave these two variables to arrive at n^2 points for calculation\n","optVar_x0, optVar_x1 = np.meshgrid(val_x0, val_x1)\n","\n","# initialize a figure container\n","fig = plt.figure(figsize=(8, 6))\n","\n","# plot contour to display objective landscape\n","plt.contour(optVar_x0, optVar_x1, rosenFW([optVar_x0, optVar_x1]))\n","\n","# plot the iteration x_star values\n","plt.scatter(x_out_sd[:,0],x_out_sd[:,1], c='b')\n","plt.plot   (x_out_sd[:,0],x_out_sd[:,1], c='b')\n","plt.scatter(x_out_cg[:,0],x_out_cg[:,1], c='r')\n","plt.plot   (x_out_cg[:,0],x_out_cg[:,1], c='r')\n","plt.scatter(x_out_mn[:,0],x_out_mn[:,1], c='g')\n","plt.plot   (x_out_mn[:,0],x_out_mn[:,1], c='g')\n","\n","plt.show()"],"metadata":{"id":"iedQEM7CYwxN"},"id":"iedQEM7CYwxN","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#######################################\n","# Test case 5\n","#######################################\n","\n","# Set all the parameters\n","from sympy import symbols\n","from sympy import lambdify\n","from sympy import Matrix\n","# Define optimization variables\n","x, y = symbols('x y')\n","\n","# Define the objective to minimize\n","fun = x**2+0.5*y**2+0.25*x*y\n","dFun = Matrix([fun.diff(x) for x in [x,y]]) \n","ddFun = Matrix([[y.diff(x) for y in dFun] for x in [x,y]]) # Calculate hessian\n","\n","\n","# lambdify the objective function and the derivative\n","funF    = lambdify([x,y], fun)\n","dFunF   = lambdify([x,y], dFun)\n","ddFunF  = lambdify([x,y], ddFun)\n","\n","# define wrapper functions\n","def funFW (x):\n","  return funF(x[0],x[1])\n","\n","def dFunFW (x):\n","  return dFunF(x[0],x[1])\n","\n","def ddFunFW (x):\n","  return ddFunF(x[0],x[1])\n","\n","# execution of your optimization code \n","x0 = np.array([-10., 10.])\n","x_out_sd = sd(funFW,x0,dFunFW,0.0000001)\n","print('Steepest descent gives: ' + str(x_out_sd[-1]))\n","x_out_cg = cg(funFW,x0,dFunFW,0.0000001)\n","print('Conjugate gradient gives: ' + str(x_out_cg[-1]))\n","x_out_mn = mn(funFW,x0,dFunFW,ddFunFW,0.0000001)\n","print('Modified Newton gives: ' + str(x_out_mn[-1]))\n","\n","print ('Steepest Descent')\n","print (x_out_sd)\n","\n","print ('Conjugate Gradient')\n","print (x_out_cg)\n","\n","print ('Modified Newton')\n","print (x_out_mn)\n","# Check your code against scipy optimize\n","from scipy.optimize import minimize\n","res = minimize(testFun, x0)\n","print('Scipy gives: ' + str(res.x))"],"metadata":{"id":"Q5LWy9L9N2YW"},"id":"Q5LWy9L9N2YW","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#######################################\n","# Test case 5\n","#######################################\n","\n","import matplotlib.pyplot as plt\n","b = 15\n","# calculate for n data points \n","n = 100 \n","val_x = np.linspace(-b, b, n)  \n","\n","# meshgrid allows us to weave these two variables to arrive at n^2 points for calculation\n","optVar_x0, optVar_x1 = np.meshgrid(val_x, val_x)\n","\n","# initialize a figure container\n","fig = plt.figure(figsize=(8, 6))\n","\n","# plot contour to display objective landscape\n","plt.contour(optVar_x0, optVar_x1, funFW([optVar_x0, optVar_x1]))\n","\n","# plot the iteration x_star values\n","\n","plt.scatter(x_out_sd[:,0],x_out_sd[:,1], c='b')\n","plt.plot   (x_out_sd[:,0],x_out_sd[:,1], c='b')\n","plt.scatter(x_out_cg[:,0],x_out_cg[:,1], c='r')\n","plt.plot   (x_out_cg[:,0],x_out_cg[:,1], c='r')\n","plt.scatter(x_out_mn[:,0],x_out_mn[:,1], c='g')\n","plt.plot   (x_out_mn[:,0],x_out_mn[:,1], c='g')\n","\n","plt.show()"],"metadata":{"id":"psZ9rSj-ZVNq"},"id":"psZ9rSj-ZVNq","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"0tuHdtgC785I"},"id":"0tuHdtgC785I","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"},"colab":{"name":"assignment_02_solution.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}